{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asgn1_20001744.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDQeAIYZtMOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''coded and commented fully by Zili Luo\n",
        "Student No.20001744\n",
        "Net Id: 15zl35\n",
        "'''\n",
        "\n",
        "'''install some imported packages'''\n",
        "!pip install tensorflow-gpu==2.0.0rc numpy tqdm --no-cache-dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0Y40jHuxYH",
        "colab_type": "code",
        "outputId": "a600af9f-b611-4c3a-ec5d-47a5766319d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''essencial imports please check tensor version is 2.00-rc0'''\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from google.colab import files\n",
        "from random import shuffle\n",
        "import csv\n",
        "import sklearn.metrics\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHJFbI_6Lkci",
        "colab_type": "code",
        "outputId": "8ef90d18-3a75-453f-955e-a2212beb9dce",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "'''if using colab, use this function to update training data set if run on local machine just ignore this function'''\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17881383-8960-499d-91fc-c53ae75d7f60\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-17881383-8960-499d-91fc-c53ae75d7f60\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris_train.txt to iris_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cRnjcCfx95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''data processing part extract training dataset\n",
        "I use shuffle to set the tranning list at random order, this helps to improve the performance\n",
        "Variables:\n",
        "  datas:numpy.array of numpy.array\n",
        "  labels:numpy.array of array\n",
        "'''\n",
        "total = []\n",
        "datas = []\n",
        "labels = []\n",
        "with open(\"iris_train.txt\") as f:#if using other name of the training dataset please change the name\n",
        "  for line in f:\n",
        "    total.append(line.split(\",\"))\n",
        "shuffle(total)\n",
        "for item in total:\n",
        "  datas.append(np.reshape(np.asarray(item[0:-1]),(4,1)))\n",
        "  label_text = item[-1].replace(\"\\n\",\"\")\n",
        "  labels.append([[0,1][label_text==\"Iris-setosa\"],[0,1][label_text==\"Iris-versicolor\"],[0,1][label_text==\"Iris-virginica\"]])\n",
        "\n",
        "datas = np.asarray(datas,dtype='float32')\n",
        "labels = np.asarray(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naEJM-LXgh9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''loss function\n",
        "Input variables:\n",
        "  pred:tensor with shape(3,1)\n",
        "        the predict value of the output from model\n",
        "  target: tensor with shape(3,1)\n",
        "        the actual value of the output from model\n",
        "Return variables:\n",
        "  the error between actual value and the predicted value\n",
        "'''\n",
        "def loss(pred,target):\n",
        "  return target - pred\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teEXo__GgHXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''the proceptron model\n",
        "it is a 4-input with 3-output unit fully connected perceptron network(ANN) model\n",
        "State variables:\n",
        "  self.W: the weight tensor of the network with shape(4,3) all set to zeros at initialize\n",
        "  self.B: the bias tensor of the network unit with shape(3,1) all set to zeros at initialize\n",
        "  \n",
        "__Call__:\n",
        "  Input variables:\n",
        "    x:tensor with shape(4,1)\n",
        "      the input from training set\n",
        "  Return:tensor with shape(3,1)\n",
        "      the output of sigmoid activate function with the sum of mutiple of input tensor\n",
        "      and the weight tensor plus unit bias\n",
        "'''\n",
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    self.W = tf.Variable(tf.zeros([4,3]))\n",
        "    self.B = tf.Variable(tf.zeros([3]))\n",
        "    \n",
        "   \n",
        "  def __call__(self,x):\n",
        "    return tf.sigmoid(tf.math.reduce_sum(self.W * x+ self.B,[0])) #  + self.B\n",
        "\n",
        "'''the pocket model\n",
        "State variables:\n",
        "  self.W: the weight tensor of the pocket with shape(4,3)\n",
        "  self.B: the bias tensor of the pocket unit with shape(3,1)\n",
        "  self.run_W: cunnt of the cueeent in-pocket weight tensor's best length of run\n",
        "  self.run_w: cunnt of the cueeent model weight tensor's best length of run\n",
        "  self.run_succ_W: count of the current in-pocket weight tensor's total correct prediction on the whole traning set\n",
        "  \n",
        "'''\n",
        "class Pocket(object):\n",
        "  def __init__(self,model):\n",
        "    self.W = model.W\n",
        "    self.B = model.B\n",
        "    self.run_W = 0\n",
        "    self.run_w = 0\n",
        "    self.run_succ_W = 0\n",
        "\n",
        "    \n",
        "'''initialize the models'''    \n",
        "model = Model()\n",
        "pocket = Pocket(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gNfd9-6jSPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''training function\n",
        "this function is a implementation of pocket algrothim with Ratchet to improve overall performance\n",
        "and set the threshold of Perceptron to fire at 0.5 but in eval function or actual use, I use argmax to get the output.\n",
        "Since within serval experiments, argmax perform very bad in training, but if use threshold to get the actrual output,\n",
        "It will also happens more than one perceptron fires at the same time.\n",
        "\n",
        "Input Variables:\n",
        "  model: Object\n",
        "        the ANN model\n",
        "  pocket:Object\n",
        "        the pocket model\n",
        "  inputs:numpy.array of numpy.array\n",
        "        the whole input dataset on the training data, since I use Ratchet\n",
        "  output:numpy.array of numpy.array\n",
        "        the whole actual output dataset of the training dataset.\n",
        "  index:current index of the training data\n",
        "  lr: learning rate of current epoch\n",
        "\n",
        "Return:\n",
        "  None\n",
        "'''\n",
        "def train(model,pocket,inputs,outputs,index,lr):\n",
        "  value = model(inputs[index])\n",
        "  a = np.array([1 if value[i] > 0.5 else 0 for i in range(3)])\n",
        "  current_loss = loss(a,outputs[index])\n",
        "  if np.array_equal(current_loss,np.array([0,0,0])): #if the the weight predicted right keep going\n",
        "    pocket.run_w = pocket.run_w + 1\n",
        "  else:\n",
        "    if pocket.run_w > pocket.run_W: #when the weight pridected wrong but goes further than the one in the pocket\n",
        "      run_succ_w = 0                #doing the Ratchet to check if place it in the pocket\n",
        "      for i in range(len(inputs)):\n",
        "        test_val = np.array([1 if model(inputs[i])[j] > 0.5 else 0 for j in range(3)])\n",
        "        if np.array_equal(loss(test_val,outputs[i]),np.array([0,0,0])) == 0:\n",
        "          run_succ_w = run_succ_w + 1\n",
        "      if run_succ_w > pocket.run_succ_W: #Ratchet succeed\n",
        "        pocket.W = model.W\n",
        "        pocket.B = model.B\n",
        "        pocket.run_succ_W = run_succ_w\n",
        "        pocket.run_W = pocket.run_w\n",
        "    model.W = model.W + lr*current_loss*inputs[index]\n",
        "    model.B = model.B + lr*current_loss*inputs[index]\n",
        "    pocket.run_w = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8DjCc1iu9nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''training steps\n",
        "just some codes to call the training function\n",
        "I set the learning rate to 0.2 at initial and mutiple 0.99 after each epoch\n",
        "those variable performs well on the training and testing set\n",
        "'''\n",
        "step_size = len(datas)\n",
        "epochs = 200\n",
        "pbar = tqdm(range(epochs))\n",
        "step = 0\n",
        "lr = 0.2\n",
        "for epoch in pbar:\n",
        "  for step in range(step_size):\n",
        "    train(model,pocket,datas,labels,step,lr = lr)\n",
        "    pbar.set_description('Epoch %2d:'%\n",
        "          (epoch))\n",
        "  lr = lr*0.99\n",
        "  \n",
        "#after training apply the weight tensor and the bias tensor in the pocket to the model\n",
        "model.W = pocket.W\n",
        "model.B = pocket.B"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv0b3hFi4w0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.print(pocket.W)\n",
        "tf.print(pocket.B)\n",
        "'''weights and bias in the report to apply just uncomment following codes'''\n",
        "# previous = np.asarray([[0.98,4.55372334,-9.00216866],[1.54,-5.41183615,-9.5839529],[-2.24,2.86118317,12.7408953],[-1.17999983,-6.40912533,10.7291946]])\n",
        "# model.W = previous\n",
        "# model.B = previous"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knrE69n-fZJ8",
        "colab_type": "code",
        "outputId": "df87778b-52b8-4cfa-9890-dae4efcd10c3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "'''if using colab, use this function to update testing data set if run on local machine just ignore this function\n",
        "if upload fails just give it another try\n",
        "'''\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ea6f180-fce8-4527-885d-2b31de5bf0a0\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5ea6f180-fce8-4527-885d-2b31de5bf0a0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris_test.txt to iris_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mk0iNThyY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''data processing part extract testing dataset\n",
        "Variables:\n",
        "  datas_test:numpy.array of numpy.array\n",
        "  labels_test:numpy.array of array\n",
        "'''\n",
        "datas_test = []\n",
        "labels_test = []\n",
        "datas_test_origin = []\n",
        "with open(\"iris_test.txt\") as f:\n",
        "  for line in f:\n",
        "    data = line.split(\",\")\n",
        "    datas_test_origin.append(data[0:-1])\n",
        "    datas_test.append(np.reshape(np.asarray(data[0:-1]),(4,1)))\n",
        "    label_text = data[-1].replace(\"\\n\",\"\")\n",
        "    labels_test.append([[0,1][label_text==\"Iris-setosa\"],[0,1][label_text==\"Iris-versicolor\"],[0,1][label_text==\"Iris-virginica\"]])\n",
        "\n",
        "datas_test = np.asarray(datas_test,dtype='float32')\n",
        "labels_test = np.asarray(labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1IUNlvvU88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''get the predicted output from the model and write the file\n",
        "If download fails just give it another try\n",
        "labels_prd_train:list\n",
        "            used for confusion matrix\n",
        "labels_act_train:list\n",
        "            used for confusion matrix\n",
        "labels_prd:list\n",
        "            used for confusion matrix\n",
        "labels_act:list\n",
        "            used for confusion matrix\n",
        "'''\n",
        "labels_prd_train = []\n",
        "labels_act_train = []\n",
        "labels_prd = []\n",
        "labels_act = []\n",
        "with open(\"result.csv\",\"w+\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow([\"Sepal length\"]+[\"Sepal width\"]+[\"Petal length\"]+[\"Petal width\"]+[\"Actual class label\"]+[\"Predicted class label\"])\n",
        "  for i in range(len(datas_test)):\n",
        "    value = model(datas_test[i])\n",
        "    label_act = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(labels_test[i]) else 0 for j in range(3)])]\n",
        "    label_prd = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(value) else 0 for j in range(3)])]\n",
        "    labels_act.append(label_act)\n",
        "    labels_prd.append(label_prd)\n",
        "    writer.writerow(datas_test_origin[i]+[label_act]+[label_prd])\n",
        "labels_act = np.asarray(labels_act)\n",
        "labels_prd = np.asarray(labels_prd)\n",
        "\n",
        "for i in range(len(datas)):\n",
        "  value = model(datas[i])\n",
        "  label_act_train = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(labels[i]) else 0 for j in range(3)])]\n",
        "  label_prd_train = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(value) else 0 for j in range(3)])]\n",
        "  labels_act_train.append(label_act_train)\n",
        "  labels_prd_train.append(label_prd_train)\n",
        "labels_act_train = np.asarray(labels_act_train)\n",
        "labels_prd_train = np.asarray(labels_prd_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqHTryx-4ROi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''after all of those download the csv file if fails just give it another try'''\n",
        "files.download(\"result.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PUAXHnozVmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_recall_martix(act,prd):\n",
        "  index_dict={\"Iris-setosa\":0,\"Iris-versicolor\":1,\"Iris-virginica\":2}\n",
        "  matrix = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
        "  for i in range(len(act)):\n",
        "    matrix[index_dict[act[i]]][index_dict[prd[i]]] += 1\n",
        "  print(\"---------------------confusion matrix-----------------------------\")\n",
        "  print(matrix)\n",
        "  print(\"---------------------precision recall-----------------------------\")\n",
        "  presion_recall = np.array([[0,0],[0,0],[0,0]],dtype=\"float32\")\n",
        "  for i in range(3):\n",
        "    presion_recall[i][0] = matrix[i][i]/(matrix[0][i]+matrix[1][i]+matrix[2][i])\n",
        "    presion_recall[i][1] = matrix[i][i]/(len(act)/3)\n",
        "  print(presion_recall)\n",
        "#   \n",
        "#   for i in range(3):\n",
        "#     presion_recall[i][0] = matrix_pr[i][i]\n",
        "#     for j in range(3):\n",
        "#       if i != j:\n",
        "#          presion_recall[i][1] += matrix_pr[i][j]\n",
        "#   for i in range(3):\n",
        "#     presion_recall[i][1] /= matrix_pr[i][i]\n",
        "#   print( presion_recall)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDKmYnWX8Mcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "20c197cf-dc36-41b4-c80b-939a65179642"
      },
      "source": [
        "print(\"-----------------------------training-------------------------------\")\n",
        "precision_recall_martix(labels_act_train,labels_prd_train)\n",
        "print(\"-----------------------------testing-------------------------------\")\n",
        "precision_recall_martix(labels_act,labels_prd)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------training-------------------------------\n",
            "---------------------confusion matrix-----------------------------\n",
            "[[38  2  0]\n",
            " [ 0 40  0]\n",
            " [ 0 26 14]]\n",
            "---------------------precision recall-----------------------------\n",
            "[[1.        0.95     ]\n",
            " [0.5882353 1.       ]\n",
            " [1.        0.35     ]]\n",
            "-----------------------------testing-------------------------------\n",
            "---------------------confusion matrix-----------------------------\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  7  3]]\n",
            "---------------------precision recall-----------------------------\n",
            "[[1.        1.       ]\n",
            " [0.5882353 1.       ]\n",
            " [1.        0.3      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCuE0mUpiXLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# '''precision-recall and confusin matrix by tools'''\n",
        "# print(\"---------------------precision recall-----------------------------\")\n",
        "# print(sklearn.metrics.classification_report(labels_act,labels_prd))\n",
        "# print(\"---------------------confusion matrix-----------------------------\")\n",
        "# print(sklearn.metrics.multilabel_confusion_matrix(labels_act,labels_prd))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXaxrgTd-dKf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7fR5QZ-ZQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''the tool based model part\n",
        "I select keras to build model with the same structure\n",
        "but as the tool do not provide the pocket algorithm, \n",
        "I choose the as SGD optimaizer,CategoricalCrossentropy as loss function\n",
        "And softmax as activation function\n",
        "'''\n",
        "\n",
        "\n",
        "'''build up the same 4-input 3-output model by keras'''\n",
        "model_keras = tf.keras.Sequential()\n",
        "model_keras.add(tf.keras.layers.Dense(3,input_shape=(4,),activation=tf.nn.softmax))\n",
        "\n",
        "model_keras.compile(optimizer='sgd',loss = tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js_7Ct0pGqPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''make the shape of the inputs fit the outputs'''\n",
        "datas_keras =[]\n",
        "for item in datas:\n",
        "  datas_keras.append(item.reshape(4,))\n",
        "datas_keras = np.asarray(datas_keras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgFKVIbo8-3q",
        "colab_type": "code",
        "outputId": "2749b78e-8112-4f2b-dde4-0c20f8e276a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''train the model'''\n",
        "model_keras.fit(x=datas_keras,y=labels,epochs=200)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 120 samples\n",
            "Epoch 1/200\n",
            "120/120 [==============================] - 0s 3ms/sample - loss: 8.9137 - accuracy: 0.3333\n",
            "Epoch 2/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 7.3012 - accuracy: 0.3333\n",
            "Epoch 3/200\n",
            "120/120 [==============================] - 0s 83us/sample - loss: 5.7194 - accuracy: 0.3333\n",
            "Epoch 4/200\n",
            "120/120 [==============================] - 0s 105us/sample - loss: 4.1539 - accuracy: 0.3333\n",
            "Epoch 5/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 2.8807 - accuracy: 0.3333\n",
            "Epoch 6/200\n",
            "120/120 [==============================] - 0s 84us/sample - loss: 2.1291 - accuracy: 0.3333\n",
            "Epoch 7/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 1.7795 - accuracy: 0.3250\n",
            "Epoch 8/200\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 1.6045 - accuracy: 0.2667\n",
            "Epoch 9/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 1.4997 - accuracy: 0.2333\n",
            "Epoch 10/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 1.4249 - accuracy: 0.2583\n",
            "Epoch 11/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 1.3681 - accuracy: 0.2917\n",
            "Epoch 12/200\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 1.3295 - accuracy: 0.2583\n",
            "Epoch 13/200\n",
            "120/120 [==============================] - 0s 89us/sample - loss: 1.2820 - accuracy: 0.3167\n",
            "Epoch 14/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 1.2471 - accuracy: 0.2917\n",
            "Epoch 15/200\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 1.2086 - accuracy: 0.3333\n",
            "Epoch 16/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 1.1739 - accuracy: 0.2750\n",
            "Epoch 17/200\n",
            "120/120 [==============================] - 0s 95us/sample - loss: 1.1485 - accuracy: 0.3417\n",
            "Epoch 18/200\n",
            "120/120 [==============================] - 0s 106us/sample - loss: 1.1091 - accuracy: 0.3750\n",
            "Epoch 19/200\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 1.0750 - accuracy: 0.3417\n",
            "Epoch 20/200\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 1.0516 - accuracy: 0.3583\n",
            "Epoch 21/200\n",
            "120/120 [==============================] - 0s 88us/sample - loss: 1.0276 - accuracy: 0.3333\n",
            "Epoch 22/200\n",
            "120/120 [==============================] - 0s 80us/sample - loss: 0.9953 - accuracy: 0.4667\n",
            "Epoch 23/200\n",
            "120/120 [==============================] - 0s 105us/sample - loss: 0.9729 - accuracy: 0.4917\n",
            "Epoch 24/200\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.9510 - accuracy: 0.5167\n",
            "Epoch 25/200\n",
            "120/120 [==============================] - 0s 108us/sample - loss: 0.9334 - accuracy: 0.6167\n",
            "Epoch 26/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.9086 - accuracy: 0.6833\n",
            "Epoch 27/200\n",
            "120/120 [==============================] - 0s 91us/sample - loss: 0.8903 - accuracy: 0.6500\n",
            "Epoch 28/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.8762 - accuracy: 0.6750\n",
            "Epoch 29/200\n",
            "120/120 [==============================] - 0s 80us/sample - loss: 0.8583 - accuracy: 0.7333\n",
            "Epoch 30/200\n",
            "120/120 [==============================] - 0s 86us/sample - loss: 0.8446 - accuracy: 0.7167\n",
            "Epoch 31/200\n",
            "120/120 [==============================] - 0s 103us/sample - loss: 0.8240 - accuracy: 0.7667\n",
            "Epoch 32/200\n",
            "120/120 [==============================] - 0s 107us/sample - loss: 0.8092 - accuracy: 0.7750\n",
            "Epoch 33/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.7956 - accuracy: 0.7250\n",
            "Epoch 34/200\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.7878 - accuracy: 0.7667\n",
            "Epoch 35/200\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.7741 - accuracy: 0.7750\n",
            "Epoch 36/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.7625 - accuracy: 0.7500\n",
            "Epoch 37/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.7484 - accuracy: 0.7667\n",
            "Epoch 38/200\n",
            "120/120 [==============================] - 0s 103us/sample - loss: 0.7381 - accuracy: 0.7750\n",
            "Epoch 39/200\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.7377 - accuracy: 0.7000\n",
            "Epoch 40/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.7262 - accuracy: 0.7750\n",
            "Epoch 41/200\n",
            "120/120 [==============================] - 0s 108us/sample - loss: 0.7110 - accuracy: 0.7833\n",
            "Epoch 42/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.7043 - accuracy: 0.7833\n",
            "Epoch 43/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 0.6998 - accuracy: 0.7667\n",
            "Epoch 44/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.6871 - accuracy: 0.8000\n",
            "Epoch 45/200\n",
            "120/120 [==============================] - 0s 142us/sample - loss: 0.6796 - accuracy: 0.8000\n",
            "Epoch 46/200\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.6726 - accuracy: 0.7917\n",
            "Epoch 47/200\n",
            "120/120 [==============================] - 0s 152us/sample - loss: 0.6686 - accuracy: 0.7750\n",
            "Epoch 48/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.6587 - accuracy: 0.8083\n",
            "Epoch 49/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.6523 - accuracy: 0.7917\n",
            "Epoch 50/200\n",
            "120/120 [==============================] - 0s 128us/sample - loss: 0.6472 - accuracy: 0.8000\n",
            "Epoch 51/200\n",
            "120/120 [==============================] - 0s 108us/sample - loss: 0.6415 - accuracy: 0.8083\n",
            "Epoch 52/200\n",
            "120/120 [==============================] - 0s 149us/sample - loss: 0.6351 - accuracy: 0.8083\n",
            "Epoch 53/200\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.6303 - accuracy: 0.8250\n",
            "Epoch 54/200\n",
            "120/120 [==============================] - 0s 168us/sample - loss: 0.6246 - accuracy: 0.8250\n",
            "Epoch 55/200\n",
            "120/120 [==============================] - 0s 103us/sample - loss: 0.6192 - accuracy: 0.8250\n",
            "Epoch 56/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.6137 - accuracy: 0.8333\n",
            "Epoch 57/200\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 0.6094 - accuracy: 0.8167\n",
            "Epoch 58/200\n",
            "120/120 [==============================] - 0s 98us/sample - loss: 0.6061 - accuracy: 0.8250\n",
            "Epoch 59/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.6073 - accuracy: 0.7917\n",
            "Epoch 60/200\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.5970 - accuracy: 0.8333\n",
            "Epoch 61/200\n",
            "120/120 [==============================] - 0s 86us/sample - loss: 0.5947 - accuracy: 0.8500\n",
            "Epoch 62/200\n",
            "120/120 [==============================] - 0s 105us/sample - loss: 0.5872 - accuracy: 0.8167\n",
            "Epoch 63/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.5834 - accuracy: 0.8333\n",
            "Epoch 64/200\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.5799 - accuracy: 0.8250\n",
            "Epoch 65/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.5812 - accuracy: 0.8083\n",
            "Epoch 66/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 0.5817 - accuracy: 0.8000\n",
            "Epoch 67/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 0.5697 - accuracy: 0.8333\n",
            "Epoch 68/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.5684 - accuracy: 0.8417\n",
            "Epoch 69/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.5678 - accuracy: 0.8000\n",
            "Epoch 70/200\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.5592 - accuracy: 0.8333\n",
            "Epoch 71/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 0.5561 - accuracy: 0.8417\n",
            "Epoch 72/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.5564 - accuracy: 0.8333\n",
            "Epoch 73/200\n",
            "120/120 [==============================] - 0s 102us/sample - loss: 0.5533 - accuracy: 0.8167\n",
            "Epoch 74/200\n",
            "120/120 [==============================] - 0s 75us/sample - loss: 0.5498 - accuracy: 0.8667\n",
            "Epoch 75/200\n",
            "120/120 [==============================] - 0s 83us/sample - loss: 0.5459 - accuracy: 0.8667\n",
            "Epoch 76/200\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.5421 - accuracy: 0.8500\n",
            "Epoch 77/200\n",
            "120/120 [==============================] - 0s 74us/sample - loss: 0.5415 - accuracy: 0.8500\n",
            "Epoch 78/200\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.5369 - accuracy: 0.8333\n",
            "Epoch 79/200\n",
            "120/120 [==============================] - 0s 107us/sample - loss: 0.5366 - accuracy: 0.8500\n",
            "Epoch 80/200\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.5316 - accuracy: 0.8750\n",
            "Epoch 81/200\n",
            "120/120 [==============================] - 0s 179us/sample - loss: 0.5305 - accuracy: 0.8417\n",
            "Epoch 82/200\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.5271 - accuracy: 0.8667\n",
            "Epoch 83/200\n",
            "120/120 [==============================] - 0s 94us/sample - loss: 0.5290 - accuracy: 0.8583\n",
            "Epoch 84/200\n",
            "120/120 [==============================] - 0s 123us/sample - loss: 0.5215 - accuracy: 0.9167\n",
            "Epoch 85/200\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.5189 - accuracy: 0.8583\n",
            "Epoch 86/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.5174 - accuracy: 0.8583\n",
            "Epoch 87/200\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.5181 - accuracy: 0.8500\n",
            "Epoch 88/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.5122 - accuracy: 0.8750\n",
            "Epoch 89/200\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.5112 - accuracy: 0.8500\n",
            "Epoch 90/200\n",
            "120/120 [==============================] - 0s 118us/sample - loss: 0.5094 - accuracy: 0.8667\n",
            "Epoch 91/200\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.5083 - accuracy: 0.8750\n",
            "Epoch 92/200\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.5066 - accuracy: 0.9000\n",
            "Epoch 93/200\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.5046 - accuracy: 0.8250\n",
            "Epoch 94/200\n",
            "120/120 [==============================] - 0s 82us/sample - loss: 0.5023 - accuracy: 0.9083\n",
            "Epoch 95/200\n",
            "120/120 [==============================] - 0s 77us/sample - loss: 0.5021 - accuracy: 0.8667\n",
            "Epoch 96/200\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.4977 - accuracy: 0.8917\n",
            "Epoch 97/200\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.5009 - accuracy: 0.8583\n",
            "Epoch 98/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.4976 - accuracy: 0.8917\n",
            "Epoch 99/200\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.4920 - accuracy: 0.9083\n",
            "Epoch 100/200\n",
            "120/120 [==============================] - 0s 140us/sample - loss: 0.4901 - accuracy: 0.9083\n",
            "Epoch 101/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.4905 - accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.4887 - accuracy: 0.9000\n",
            "Epoch 103/200\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.4852 - accuracy: 0.8583\n",
            "Epoch 104/200\n",
            "120/120 [==============================] - 0s 86us/sample - loss: 0.4842 - accuracy: 0.8833\n",
            "Epoch 105/200\n",
            "120/120 [==============================] - 0s 102us/sample - loss: 0.4884 - accuracy: 0.8917\n",
            "Epoch 106/200\n",
            "120/120 [==============================] - 0s 84us/sample - loss: 0.4868 - accuracy: 0.8833\n",
            "Epoch 107/200\n",
            "120/120 [==============================] - 0s 94us/sample - loss: 0.4789 - accuracy: 0.8750\n",
            "Epoch 108/200\n",
            "120/120 [==============================] - 0s 103us/sample - loss: 0.4783 - accuracy: 0.8917\n",
            "Epoch 109/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.4780 - accuracy: 0.8583\n",
            "Epoch 110/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.4737 - accuracy: 0.9000\n",
            "Epoch 111/200\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.4761 - accuracy: 0.9000\n",
            "Epoch 112/200\n",
            "120/120 [==============================] - 0s 122us/sample - loss: 0.4764 - accuracy: 0.8583\n",
            "Epoch 113/200\n",
            "120/120 [==============================] - 0s 159us/sample - loss: 0.4702 - accuracy: 0.8583\n",
            "Epoch 114/200\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.4709 - accuracy: 0.8917\n",
            "Epoch 115/200\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.4693 - accuracy: 0.9000\n",
            "Epoch 116/200\n",
            "120/120 [==============================] - 0s 117us/sample - loss: 0.4654 - accuracy: 0.9250\n",
            "Epoch 117/200\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.4637 - accuracy: 0.8917\n",
            "Epoch 118/200\n",
            "120/120 [==============================] - 0s 135us/sample - loss: 0.4646 - accuracy: 0.8917\n",
            "Epoch 119/200\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.4613 - accuracy: 0.9333\n",
            "Epoch 120/200\n",
            "120/120 [==============================] - 0s 164us/sample - loss: 0.4597 - accuracy: 0.9167\n",
            "Epoch 121/200\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.4607 - accuracy: 0.9167\n",
            "Epoch 122/200\n",
            "120/120 [==============================] - 0s 114us/sample - loss: 0.4581 - accuracy: 0.8917\n",
            "Epoch 123/200\n",
            "120/120 [==============================] - 0s 114us/sample - loss: 0.4603 - accuracy: 0.9250\n",
            "Epoch 124/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.4557 - accuracy: 0.9083\n",
            "Epoch 125/200\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.4574 - accuracy: 0.9167\n",
            "Epoch 126/200\n",
            "120/120 [==============================] - 0s 88us/sample - loss: 0.4545 - accuracy: 0.9167\n",
            "Epoch 127/200\n",
            "120/120 [==============================] - 0s 92us/sample - loss: 0.4512 - accuracy: 0.9083\n",
            "Epoch 128/200\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.4529 - accuracy: 0.9250\n",
            "Epoch 129/200\n",
            "120/120 [==============================] - 0s 81us/sample - loss: 0.4500 - accuracy: 0.9333\n",
            "Epoch 130/200\n",
            "120/120 [==============================] - 0s 143us/sample - loss: 0.4480 - accuracy: 0.9083\n",
            "Epoch 131/200\n",
            "120/120 [==============================] - 0s 120us/sample - loss: 0.4471 - accuracy: 0.9167\n",
            "Epoch 132/200\n",
            "120/120 [==============================] - 0s 110us/sample - loss: 0.4484 - accuracy: 0.9250\n",
            "Epoch 133/200\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.4455 - accuracy: 0.9000\n",
            "Epoch 134/200\n",
            "120/120 [==============================] - 0s 107us/sample - loss: 0.4476 - accuracy: 0.9167\n",
            "Epoch 135/200\n",
            "120/120 [==============================] - 0s 91us/sample - loss: 0.4429 - accuracy: 0.9167\n",
            "Epoch 136/200\n",
            "120/120 [==============================] - 0s 107us/sample - loss: 0.4413 - accuracy: 0.9333\n",
            "Epoch 137/200\n",
            "120/120 [==============================] - 0s 88us/sample - loss: 0.4397 - accuracy: 0.9083\n",
            "Epoch 138/200\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.4385 - accuracy: 0.9083\n",
            "Epoch 139/200\n",
            "120/120 [==============================] - 0s 78us/sample - loss: 0.4422 - accuracy: 0.9083\n",
            "Epoch 140/200\n",
            "120/120 [==============================] - 0s 94us/sample - loss: 0.4420 - accuracy: 0.9083\n",
            "Epoch 141/200\n",
            "120/120 [==============================] - 0s 109us/sample - loss: 0.4384 - accuracy: 0.9250\n",
            "Epoch 142/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.4397 - accuracy: 0.9333\n",
            "Epoch 143/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.4336 - accuracy: 0.9250\n",
            "Epoch 144/200\n",
            "120/120 [==============================] - 0s 116us/sample - loss: 0.4323 - accuracy: 0.9333\n",
            "Epoch 145/200\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.4312 - accuracy: 0.9333\n",
            "Epoch 146/200\n",
            "120/120 [==============================] - 0s 126us/sample - loss: 0.4311 - accuracy: 0.9167\n",
            "Epoch 147/200\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.4378 - accuracy: 0.9250\n",
            "Epoch 148/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.4289 - accuracy: 0.9333\n",
            "Epoch 149/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.4304 - accuracy: 0.9250\n",
            "Epoch 150/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.4277 - accuracy: 0.9500\n",
            "Epoch 151/200\n",
            "120/120 [==============================] - 0s 93us/sample - loss: 0.4266 - accuracy: 0.9333\n",
            "Epoch 152/200\n",
            "120/120 [==============================] - 0s 95us/sample - loss: 0.4237 - accuracy: 0.9500\n",
            "Epoch 153/200\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.4245 - accuracy: 0.9250\n",
            "Epoch 154/200\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.4230 - accuracy: 0.9417\n",
            "Epoch 155/200\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.4242 - accuracy: 0.9083\n",
            "Epoch 156/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.4260 - accuracy: 0.9000\n",
            "Epoch 157/200\n",
            "120/120 [==============================] - 0s 84us/sample - loss: 0.4207 - accuracy: 0.9250\n",
            "Epoch 158/200\n",
            "120/120 [==============================] - 0s 102us/sample - loss: 0.4217 - accuracy: 0.9250\n",
            "Epoch 159/200\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.4198 - accuracy: 0.8917\n",
            "Epoch 160/200\n",
            "120/120 [==============================] - 0s 107us/sample - loss: 0.4173 - accuracy: 0.9250\n",
            "Epoch 161/200\n",
            "120/120 [==============================] - 0s 121us/sample - loss: 0.4169 - accuracy: 0.9500\n",
            "Epoch 162/200\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.4153 - accuracy: 0.9500\n",
            "Epoch 163/200\n",
            "120/120 [==============================] - 0s 115us/sample - loss: 0.4172 - accuracy: 0.9167\n",
            "Epoch 164/200\n",
            "120/120 [==============================] - 0s 88us/sample - loss: 0.4150 - accuracy: 0.9333\n",
            "Epoch 165/200\n",
            "120/120 [==============================] - 0s 106us/sample - loss: 0.4169 - accuracy: 0.9333\n",
            "Epoch 166/200\n",
            "120/120 [==============================] - 0s 85us/sample - loss: 0.4135 - accuracy: 0.9583\n",
            "Epoch 167/200\n",
            "120/120 [==============================] - 0s 72us/sample - loss: 0.4128 - accuracy: 0.9250\n",
            "Epoch 168/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.4104 - accuracy: 0.9333\n",
            "Epoch 169/200\n",
            "120/120 [==============================] - 0s 127us/sample - loss: 0.4127 - accuracy: 0.9333\n",
            "Epoch 170/200\n",
            "120/120 [==============================] - 0s 89us/sample - loss: 0.4113 - accuracy: 0.9250\n",
            "Epoch 171/200\n",
            "120/120 [==============================] - 0s 87us/sample - loss: 0.4086 - accuracy: 0.9000\n",
            "Epoch 172/200\n",
            "120/120 [==============================] - 0s 93us/sample - loss: 0.4085 - accuracy: 0.9417\n",
            "Epoch 173/200\n",
            "120/120 [==============================] - 0s 91us/sample - loss: 0.4140 - accuracy: 0.8917\n",
            "Epoch 174/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.4066 - accuracy: 0.9333\n",
            "Epoch 175/200\n",
            "120/120 [==============================] - 0s 95us/sample - loss: 0.4048 - accuracy: 0.9500\n",
            "Epoch 176/200\n",
            "120/120 [==============================] - 0s 95us/sample - loss: 0.4060 - accuracy: 0.9333\n",
            "Epoch 177/200\n",
            "120/120 [==============================] - 0s 99us/sample - loss: 0.4048 - accuracy: 0.9417\n",
            "Epoch 178/200\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.4026 - accuracy: 0.9250\n",
            "Epoch 179/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.4016 - accuracy: 0.9500\n",
            "Epoch 180/200\n",
            "120/120 [==============================] - 0s 81us/sample - loss: 0.4005 - accuracy: 0.9500\n",
            "Epoch 181/200\n",
            "120/120 [==============================] - 0s 108us/sample - loss: 0.4007 - accuracy: 0.9333\n",
            "Epoch 182/200\n",
            "120/120 [==============================] - 0s 82us/sample - loss: 0.4027 - accuracy: 0.9167\n",
            "Epoch 183/200\n",
            "120/120 [==============================] - 0s 103us/sample - loss: 0.4007 - accuracy: 0.9333\n",
            "Epoch 184/200\n",
            "120/120 [==============================] - 0s 96us/sample - loss: 0.3982 - accuracy: 0.9500\n",
            "Epoch 185/200\n",
            "120/120 [==============================] - 0s 75us/sample - loss: 0.3980 - accuracy: 0.9250\n",
            "Epoch 186/200\n",
            "120/120 [==============================] - 0s 79us/sample - loss: 0.3963 - accuracy: 0.9417\n",
            "Epoch 187/200\n",
            "120/120 [==============================] - 0s 81us/sample - loss: 0.3966 - accuracy: 0.9500\n",
            "Epoch 188/200\n",
            "120/120 [==============================] - 0s 84us/sample - loss: 0.3946 - accuracy: 0.9583\n",
            "Epoch 189/200\n",
            "120/120 [==============================] - 0s 104us/sample - loss: 0.3942 - accuracy: 0.9500\n",
            "Epoch 190/200\n",
            "120/120 [==============================] - 0s 100us/sample - loss: 0.3939 - accuracy: 0.9583\n",
            "Epoch 191/200\n",
            "120/120 [==============================] - 0s 119us/sample - loss: 0.3920 - accuracy: 0.9583\n",
            "Epoch 192/200\n",
            "120/120 [==============================] - 0s 112us/sample - loss: 0.3956 - accuracy: 0.9500\n",
            "Epoch 193/200\n",
            "120/120 [==============================] - 0s 111us/sample - loss: 0.3903 - accuracy: 0.9500\n",
            "Epoch 194/200\n",
            "120/120 [==============================] - 0s 97us/sample - loss: 0.3905 - accuracy: 0.9500\n",
            "Epoch 195/200\n",
            "120/120 [==============================] - 0s 106us/sample - loss: 0.3889 - accuracy: 0.9500\n",
            "Epoch 196/200\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 0.3882 - accuracy: 0.9500\n",
            "Epoch 197/200\n",
            "120/120 [==============================] - 0s 101us/sample - loss: 0.3877 - accuracy: 0.9500\n",
            "Epoch 198/200\n",
            "120/120 [==============================] - 0s 113us/sample - loss: 0.3902 - accuracy: 0.9417\n",
            "Epoch 199/200\n",
            "120/120 [==============================] - 0s 108us/sample - loss: 0.3878 - accuracy: 0.9583\n",
            "Epoch 200/200\n",
            "120/120 [==============================] - 0s 90us/sample - loss: 0.3866 - accuracy: 0.9500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9320bb2240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8_DlYoH_FFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''get the predicted output from the model\n",
        "labels_prd_keras_train:list\n",
        "            used for confusion matrix\n",
        "labels_prd_keras:list\n",
        "            used for confusion matrix\n",
        "labels_act_keras:list\n",
        "            used for confusion matrix\n",
        "'''\n",
        "labels_prd_keras_train = []\n",
        "labels_prd_keras = []\n",
        "labels_act_keras = []\n",
        "for i in range(len(datas_test)):\n",
        "  value = model_keras.predict(datas_test[i].reshape(1,4)).reshape(3,1)\n",
        "  label_act_keras = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(labels_test[i]) else 0 for j in range(3)])]\n",
        "  label_prd_keras = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(value) else 0 for j in range(3)])]\n",
        "  labels_act_keras.append(label_act_keras)\n",
        "  labels_prd_keras.append(label_prd_keras)\n",
        "labels_prd_keras = np.asarray(labels_prd_keras)\n",
        "\n",
        "for i in range(len(datas)):\n",
        "  value = model_keras.predict(datas[i].reshape(1,4)).reshape(3,1)\n",
        "  label_prd_keras_train = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"][tf.argmax([1 if j == tf.math.argmax(value) else 0 for j in range(3)])]\n",
        "  labels_prd_keras_train.append(label_prd_keras_train)\n",
        "labels_prd_keras_train = np.asarray(labels_prd_keras_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxO1it0pDJp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "0cb1b035-c4e4-4014-b5e7-4b6f39cf745e"
      },
      "source": [
        "print(\"-----------------------------training-------------------------------\")\n",
        "precision_recall_martix(labels_act_train,labels_prd_keras_train)\n",
        "print(\"-----------------------------testing-------------------------------\")\n",
        "precision_recall_martix(labels_act_keras,labels_prd_keras)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------training-------------------------------\n",
            "---------------------confusion matrix-----------------------------\n",
            "[[40  0  0]\n",
            " [ 0 34  6]\n",
            " [ 0  0 40]]\n",
            "---------------------precision recall-----------------------------\n",
            "[[1.        1.       ]\n",
            " [1.        0.85     ]\n",
            " [0.8695652 1.       ]]\n",
            "-----------------------------testing-------------------------------\n",
            "---------------------confusion matrix-----------------------------\n",
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  1  9]]\n",
            "---------------------precision recall-----------------------------\n",
            "[[1.  1. ]\n",
            " [0.9 0.9]\n",
            " [0.9 0.9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bk8aCcgHxD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''precision-recall and confusin matrix by tools'''\n",
        "# print(\"---------------------precision recall-----------------------------\")\n",
        "# print(sklearn.metrics.classification_report(labels_act_keras,labels_prd_keras))\n",
        "# print(\"---------------------confusion matrix-----------------------------\")\n",
        "# print(sklearn.metrics.multilabel_confusion_matrix(labels_act_keras,labels_prd_keras))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeWHcrFGUco3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}